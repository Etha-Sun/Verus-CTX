#!/usr/bin/env python3
"""
Verusè‡ªåŠ¨è¯æ˜å™¨
ä½¿ç”¨LLMè‡ªåŠ¨ç”Ÿæˆå’Œä¿®å¤Verusè¯æ˜
"""

import os
import sys
import json
import time
import logging
import subprocess
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Dict, Optional
try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

class VerusAutoProver:
    def __init__(self, api_key: str, max_iterations: int = 5, use_gemini: bool = False):
        """
        åˆå§‹åŒ–è‡ªåŠ¨è¯æ˜å™¨
        
        Args:
            api_key: APIå¯†é’¥
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            use_gemini: æ˜¯å¦ä½¿ç”¨Gemini API
        """
        self.use_gemini = use_gemini
        self.max_iterations = max_iterations
        self.setup_logging()
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        if use_gemini:
            if genai is None:
                raise ImportError("google-generativeai package is required for Gemini API")
            genai.configure(api_key=api_key)
            self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
        else:
            if OpenAI is None:
                raise ImportError("openai package is required for OpenAI API")
            self.client = OpenAI(api_key=api_key)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_iterations': 0,
            'z3_counterexample_generated': False,
            'z3_counterexample_successful': False,
            'final_error_type': None,  # 'compilation_error', 'verification_error', 'success'
            'compilation_errors': 0,
            'verification_errors': 0,
            # Tokenç»Ÿè®¡
            'total_prompt_tokens': 0,
            'total_completion_tokens': 0,
            'total_tokens': 0,
            'api_calls': 0,
            'token_usage_by_iteration': {}  # è®°å½•æ¯æ¬¡è¿­ä»£çš„tokenä½¿ç”¨æƒ…å†µ
        }
        
        # å½“å‰å¤„ç†æ–‡ä»¶çš„CTXæ–‡ä»¶å¤¹è·¯å¾„
        self.current_ctx_folder = None
        
        # åŠ è½½æ¨¡å‹é…ç½®
        prompts = self.load_prompts()
        self.model_config = prompts.get("model_config", {})
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"verus_proof_log_{timestamp}.log"
        
        # é…ç½®æ—¥å¿—æ ¼å¼
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"=== Verus Auto Prover Session Started ===")
        self.logger.info(f"Log file: {log_filename}")
        self.logger.info(f"Max iterations: {self.max_iterations}")
        self.logger.info(f"Using {'Gemini' if self.use_gemini else 'OpenAI'} API")
        
    def log_interaction(self, step_type: str, iteration: int, prompt: str = "", response: str = "", error: str = ""):
        """è®°å½•äº¤äº’ä¿¡æ¯"""
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"STEP: {step_type} - Iteration {iteration}")
        self.logger.info(f"{'='*80}")
        
        # åªè®°å½•å…³é”®ä¿¡æ¯ï¼Œä¸é‡å¤è®°å½•prompt
        if step_type == "Z3_COUNTEREXAMPLE_GENERATION":
            self.logger.info(f"Generating Z3 counterexample code...")
        elif step_type == "Z3_COUNTEREXAMPLE_RESPONSE":
            self.logger.info(f"Z3 code generated successfully")
        elif step_type == "INITIAL_PROOF_GENERATION":
            self.logger.info(f"Generating initial proof...")
        elif step_type == "PROOF_FIX_GENERATION":
            self.logger.info(f"Generating proof fix...")
        elif step_type == "VERIFICATION_FAILED":
            self.logger.info(f"Verification failed")
            
        if response and step_type in ["Z3_COUNTEREXAMPLE_RESPONSE", "INITIAL_PROOF_RESPONSE", "PROOF_FIX_RESPONSE"]:
            self.logger.info(f"\n--- LM RESPONSE (truncated) ---")
            # åªè®°å½•å“åº”çš„å‰200ä¸ªå­—ç¬¦
            truncated_response = response[:200] + "..." if len(response) > 200 else response
            self.logger.info(truncated_response)
            
        if error:
            self.logger.info(f"\n--- VERUS ERROR ---")
            self.logger.info(error)
            
        self.logger.info(f"{'='*80}\n")
        
    def record_token_usage(self, response, iteration: int, step_type: str):
        """
        è®°å½•tokenä½¿ç”¨æƒ…å†µ
        
        Args:
            response: APIå“åº”
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            step_type: æ­¥éª¤ç±»å‹
        """
        if self.use_gemini:
            # Gemini APIçš„tokenç»Ÿè®¡
            if hasattr(response, 'usage_metadata'):
                usage = response.usage_metadata
                prompt_tokens = usage.prompt_token_count
                completion_tokens = usage.candidates_token_count
                total_tokens = prompt_tokens + completion_tokens
            else:
                # ä¼°ç®—tokenä½¿ç”¨
                prompt_tokens = len(response.text.split()) * 1.3  # ç²—ç•¥ä¼°ç®—
                completion_tokens = len(response.text.split()) * 1.3
                total_tokens = prompt_tokens + completion_tokens
        else:
            # OpenAI APIçš„tokenç»Ÿè®¡
            if hasattr(response, 'usage') and response.usage:
                usage = response.usage
                prompt_tokens = usage.prompt_tokens
                completion_tokens = usage.completion_tokens
                total_tokens = usage.total_tokens
            else:
                prompt_tokens = 0
                completion_tokens = 0
                total_tokens = 0
        
        # æ›´æ–°æ€»ç»Ÿè®¡
        self.stats['total_prompt_tokens'] += int(prompt_tokens)
        self.stats['total_completion_tokens'] += int(completion_tokens)
        self.stats['total_tokens'] += int(total_tokens)
        self.stats['api_calls'] += 1
        
        # æ›´æ–°è¿­ä»£ç»Ÿè®¡
        if iteration not in self.stats['token_usage_by_iteration']:
            self.stats['token_usage_by_iteration'][iteration] = {
                'prompt_tokens': 0,
                'completion_tokens': 0,
                'total_tokens': 0,
                'api_calls': 0,
                'steps': {}
            }
        
        self.stats['token_usage_by_iteration'][iteration]['prompt_tokens'] += int(prompt_tokens)
        self.stats['token_usage_by_iteration'][iteration]['completion_tokens'] += int(completion_tokens)
        self.stats['token_usage_by_iteration'][iteration]['total_tokens'] += int(total_tokens)
        self.stats['token_usage_by_iteration'][iteration]['api_calls'] += 1
        
        # æ›´æ–°æ­¥éª¤ç»Ÿè®¡
        if step_type not in self.stats['token_usage_by_iteration'][iteration]['steps']:
            self.stats['token_usage_by_iteration'][iteration]['steps'][step_type] = {
                'prompt_tokens': 0,
                'completion_tokens': 0,
                'total_tokens': 0
            }
        
        self.stats['token_usage_by_iteration'][iteration]['steps'][step_type]['prompt_tokens'] += int(prompt_tokens)
        self.stats['token_usage_by_iteration'][iteration]['steps'][step_type]['completion_tokens'] += int(completion_tokens)
        self.stats['token_usage_by_iteration'][iteration]['steps'][step_type]['total_tokens'] += int(total_tokens)
        
        # è®°å½•åˆ°æ—¥å¿—
        self.logger.info(f"Token usage for {step_type} (Iteration {iteration}):")
        self.logger.info(f"  Prompt tokens: {int(prompt_tokens)}")
        self.logger.info(f"  Completion tokens: {int(completion_tokens)}")
        self.logger.info(f"  Total tokens: {int(total_tokens)}")
        self.logger.info(f"Total token usage so far:")
        self.logger.info(f"  Total prompt tokens: {self.stats['total_prompt_tokens']}")
        self.logger.info(f"  Total completion tokens: {self.stats['total_completion_tokens']}")
        self.logger.info(f"  Total tokens: {self.stats['total_tokens']}")
        self.logger.info(f"  API calls: {self.stats['api_calls']}")
        self.logger.info(f"{'='*80}")
        
    def load_prompts(self) -> dict:
        """åŠ è½½promptæ¨¡æ¿"""
        try:
            with open('prompts.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            self.logger.error("prompts.json not found")
            return {}
        except json.JSONDecodeError as e:
            self.logger.error(f"Error parsing prompts.json: {e}")
            return {}
    
    def extract_rust_code(self, content: str) -> str:
        """ä»LLMå“åº”ä¸­æå–Rustä»£ç """
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        if '```rust' in content:
            start = content.find('```rust') + 7
            end = content.find('```', start)
            if end != -1:
                return content[start:end].strip()
        
        if '```' in content:
            start = content.find('```') + 3
            end = content.find('```', start)
            if end != -1:
                return content[start:end].strip()
        
        # å¦‚æœæ²¡æœ‰ä»£ç å—æ ‡è®°ï¼Œè¿”å›æ•´ä¸ªå†…å®¹
        return content.strip()
    
    def call_llm_with_retry(self, prompt: str, step_type: str, max_retries: int = 3) -> str:
        """
        è°ƒç”¨LLMï¼Œå¸¦é‡è¯•æœºåˆ¶
        
        Args:
            prompt: æç¤ºè¯
            step_type: æ­¥éª¤ç±»å‹
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            LLMå“åº”å†…å®¹
        """
        for attempt in range(max_retries):
            try:
                if self.use_gemini:
                    response = self.gemini_model.generate_content(prompt)
                    content = response.text.strip()
                else:
                    response = self.client.chat.completions.create(
                        model=self.model_config.get("model", "gpt-4o"),
                        messages=[{"role": "user", "content": prompt}],
                        temperature=self.model_config.get("temperature", 0.1),
                        max_tokens=self.model_config.get("max_tokens", 4000)
                    )
                    content = response.choices[0].message.content.strip()
                
                # æ£€æŸ¥æ‹’ç»æ¶ˆæ¯
                if "I'm sorry, but I can't assist with that request" in content or "I'm sorry" in content[:50]:
                    self.logger.warning(f"LLM returned rejection message on attempt {attempt + 1}")
                    if attempt < max_retries - 1:
                        simplified_prompt = self.simplify_prompt_for_retry(prompt, step_type)
                        prompt = simplified_prompt  # æ›´æ–°promptç”¨äºé‡è¯•
                        continue
                    else:
                        self.logger.error(f"All {max_retries} attempts failed for {step_type}")
                        return content
                
                return content
                
            except Exception as e:
                self.logger.error(f"LLM call failed on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                else:
                    raise e
        return ""
    
    def simplify_prompt_for_retry(self, original_prompt: str, step_type: str) -> str:
        """
        ä¸ºé‡è¯•ç”Ÿæˆç®€åŒ–çš„prompt
        
        Args:
            original_prompt: åŸå§‹prompt
            step_type: æ­¥éª¤ç±»å‹
            
        Returns:
            ç®€åŒ–çš„prompt
        """
        if step_type == "Z3_COUNTEREXAMPLE_GENERATION":
            # ç®€åŒ–çš„Z3åä¾‹ç”Ÿæˆprompt
            try:
                code_part = original_prompt.split('**Original Rust/Verus Code:**')[1].split('**Verus Verification Error:**')[0]
                error_part = original_prompt.split('**Verus Verification Error:**')[1].split('**Task:**')[0]
                
                return f"""Generate a Python program using z3-solver to find counterexamples for this verification failure:

Code: {code_part}

Error: {error_part}

Requirements:
- Use `from z3 import *`
- Create Z3 variables for function parameters
- Find concrete variable assignments that cause the failure
- Print results clearly

Generate the Z3 solver code:"""
            except:
                return original_prompt
                
        elif step_type in ["INITIAL_PROOF_GENERATION", "PROOF_FIX_GENERATION"]:
            # ç®€åŒ–çš„è¯æ˜ç”Ÿæˆprompt
            try:
                code_part = original_prompt.split('**Original Code:**')[1].split('**CRITICAL OUTPUT REQUIREMENT:**')[0]
                
                return f"""Generate Verus proof annotations for this code:

{code_part}

Add only proof annotations (invariants, assertions, proof blocks). Do not modify the original code.

Generate the complete Verus code:"""
            except:
                return original_prompt
                
        return original_prompt

    def generate_initial_proof(self, code: str) -> str:
        """ç”Ÿæˆåˆå§‹è¯æ˜"""
        try:
            prompts = self.load_prompts()
            prompt_template = prompts.get("initial_proof_generation", {}).get("template", "")
            
            if not prompt_template:
                self.logger.error("Initial proof generation prompt template not found")
                return code
            
            prompt = prompt_template.format(code=code)
            
            self.log_interaction("INITIAL_PROOF_GENERATION", 0, prompt)
            
            response_content = self.call_llm_with_retry(prompt, "INITIAL_PROOF_GENERATION")
            
            # å¤„ç†LLMæ‹’ç»çš„æƒ…å†µ
            if "I'm sorry" in response_content[:100]:
                self.logger.warning("LLM rejected the request for initial proof generation")
                return code  # è¿”å›åŸå§‹ä»£ç 
            
            # è®°å½•tokenä½¿ç”¨
            if self.use_gemini:
                # ä¸ºGeminiåˆ›å»ºæ¨¡æ‹Ÿå“åº”å¯¹è±¡
                class MockResponse:
                    def __init__(self, content):
                        self.text = content
                        self.usage_metadata = type('obj', (object,), {
                            'prompt_token_count': len(content.split()) * 1.3,
                            'candidates_token_count': len(content.split()) * 1.3
                        })()
                
                mock_response = MockResponse(response_content)
            else:
                mock_response = type('obj', (object,), {
                    'usage': type('obj', (object,), {
                        'prompt_tokens': len(prompt.split()) * 1.3,
                        'completion_tokens': len(response_content.split()) * 1.3,
                        'total_tokens': len(prompt.split()) * 1.3 + len(response_content.split()) * 1.3
                    })()
                })()
            
            self.record_token_usage(mock_response, 0, "INITIAL_PROOF_GENERATION")
            
            self.log_interaction("INITIAL_PROOF_RESPONSE", 0, "", response_content)
            
            return self.extract_rust_code(response_content)
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆåˆå§‹proofæ—¶å‡ºé”™: {e}")
            return code
    
    def fix_proof(self, current_code: str, error_message: str, history: List[str], counterexample_info: str = "", iteration: int = 0) -> str:
        """ä¿®å¤è¯æ˜"""
        try:
            prompts = self.load_prompts()
            prompt_template = prompts.get("iterative_fix", {}).get("template", "")
            
            if not prompt_template:
                self.logger.error("Iterative fix prompt template not found")
                return current_code
        
            # æ„å»ºå†å²ä¿¡æ¯
            history_text = "\n".join([f"Attempt {i+1}: {code}" for i, code in enumerate(history[-3:])])
        
            # æ„å»ºåä¾‹ä¿¡æ¯
            counterexample_section = ""
            if counterexample_info:
                counterexample_section = f"\n**Counterexample Information:**\n{counterexample_info}\n"
            
            prompt = prompt_template.format(
                history=history_text,
                current_code=current_code,
                error_message=error_message,
                counterexample_section=counterexample_section
            )
        
            self.log_interaction("PROOF_FIX_GENERATION", iteration, prompt)
            
            response_content = self.call_llm_with_retry(prompt, "PROOF_FIX_GENERATION", iteration)
            
            # å¤„ç†LLMæ‹’ç»çš„æƒ…å†µ
            if "I'm sorry" in response_content[:100]:
                self.logger.warning("LLM rejected the request for proof fix generation")
                return current_code  # è¿”å›å½“å‰ä»£ç 
            
            # è®°å½•tokenä½¿ç”¨
            if self.use_gemini:
                # ä¸ºGeminiåˆ›å»ºæ¨¡æ‹Ÿå“åº”å¯¹è±¡
                class MockResponse:
                    def __init__(self, content):
                        self.text = content
                        self.usage_metadata = type('obj', (object,), {
                            'prompt_token_count': len(content.split()) * 1.3,
                            'candidates_token_count': len(content.split()) * 1.3
                        })()
                
                mock_response = MockResponse(response_content)
            else:
                mock_response = type('obj', (object,), {
                    'usage': type('obj', (object,), {
                        'prompt_tokens': len(prompt.split()) * 1.3,
                        'completion_tokens': len(response_content.split()) * 1.3,
                        'total_tokens': len(prompt.split()) * 1.3 + len(response_content.split()) * 1.3
                    })()
                })()
            
            self.record_token_usage(mock_response, iteration, "PROOF_FIX_GENERATION")
            
            self.log_interaction("PROOF_FIX_RESPONSE", iteration, "", response_content)
            
            return self.extract_rust_code(response_content)
            
        except Exception as e:
            self.logger.error(f"ä¿®å¤proofæ—¶å‡ºé”™: {e}")
            return current_code
    
    def verify_with_verus(self, code: str, file_path: str) -> Tuple[bool, str, str]:
        """
        ä½¿ç”¨VeruséªŒè¯ä»£ç 
        
        Args:
            code: è¦éªŒè¯çš„ä»£ç 
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            (æ˜¯å¦æˆåŠŸ, è¾“å‡º, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = Path(file_path)
            temp_file.write_text(code, encoding='utf-8')
            
            # è¿è¡ŒVeruséªŒè¯
            result = subprocess.run(
                ['verus', str(temp_file)],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            success = result.returncode == 0
            output = result.stdout
            error = result.stderr
            
            return success, output, error
                
        except subprocess.TimeoutExpired:
            return False, "", "Verification timeout"
        except Exception as e:
            return False, "", f"Verification error: {e}"
    
    def is_compilation_error(self, error_message: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç¼–è¯‘é”™è¯¯"""
        compilation_keywords = [
            'error[E', 'syntax error', 'type error',
            'mismatched types', 'expected', 'found',
            'unknown start of token', 'expected one of',
            'missing', 'unexpected', 'invalid',
            'cannot find', 'use of moved value', 'cannot borrow'
        ]
        
        # VeruséªŒè¯é”™è¯¯å…³é”®è¯ï¼ˆä¸åº”è¯¥è¢«åˆ†ç±»ä¸ºç¼–è¯‘é”™è¯¯ï¼‰
        verus_verification_keywords = [
            'postcondition not satisfied',
            'precondition not satisfied', 
            'invariant not satisfied',
            'assertion failed',
            'verification failed',
            'loop must have a decreases clause'
        ]
        
        error_lower = error_message.lower()
        
        # å¦‚æœåŒ…å«VeruséªŒè¯é”™è¯¯å…³é”®è¯ï¼Œåˆ™ä¸æ˜¯ç¼–è¯‘é”™è¯¯
        if any(keyword.lower() in error_lower for keyword in verus_verification_keywords):
            return False
            
        # åªæœ‰åŒ…å«çœŸæ­£çš„ç¼–è¯‘é”™è¯¯å…³é”®è¯æ‰è®¤ä¸ºæ˜¯ç¼–è¯‘é”™è¯¯
        return any(keyword.lower() in error_lower for keyword in compilation_keywords)

    def needs_counterexample_generation(self, error_message: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆåä¾‹"""
        # ç¼–è¯‘é”™è¯¯ä¸éœ€è¦ç”Ÿæˆåä¾‹
        if self.is_compilation_error(error_message):
            return False
        
        # éªŒè¯é”™è¯¯éœ€è¦ç”Ÿæˆåä¾‹
        verification_keywords = [
            'postcondition not satisfied',
            'precondition not satisfied',
            'invariant not satisfied',
            'assertion failed',
            'verification failed'
        ]
        
        error_lower = error_message.lower()
        return any(keyword.lower() in error_lower for keyword in verification_keywords)

    def generate_z3_counterexample(self, code: str, error_message: str, iteration: int = 0) -> str:
        """ç”ŸæˆZ3åä¾‹ä»£ç """
        try:
            prompts = self.load_prompts()
            prompt_template = prompts.get("z3_counterexample_generation", {}).get("template", "")
            
            if not prompt_template:
                self.logger.error("Z3 counterexample generation prompt template not found")
                return ""
            
            prompt = prompt_template.format(
                current_code=code,
                error_message=error_message
            )
        
            self.log_interaction("Z3_COUNTEREXAMPLE_GENERATION", iteration, prompt)
            
            response_content = self.call_llm_with_retry(prompt, "Z3_COUNTEREXAMPLE_GENERATION", iteration)
            
            # å¤„ç†LLMæ‹’ç»çš„æƒ…å†µ
            if "I'm sorry" in response_content[:100]:
                self.logger.warning("LLM rejected the request for Z3 counterexample generation")
                return ""
            
            # è®°å½•tokenä½¿ç”¨
            if self.use_gemini:
                # ä¸ºGeminiåˆ›å»ºæ¨¡æ‹Ÿå“åº”å¯¹è±¡
                class MockResponse:
                    def __init__(self, content):
                        self.text = content
                        self.usage_metadata = type('obj', (object,), {
                            'prompt_token_count': len(content.split()) * 1.3,
                            'candidates_token_count': len(content.split()) * 1.3
                        })()
                
                mock_response = MockResponse(response_content)
            else:
                mock_response = type('obj', (object,), {
                    'usage': type('obj', (object,), {
                        'prompt_tokens': len(prompt.split()) * 1.3,
                        'completion_tokens': len(response_content.split()) * 1.3,
                        'total_tokens': len(prompt.split()) * 1.3 + len(response_content.split()) * 1.3
                    })()
                })()
            
            self.record_token_usage(mock_response, iteration, "Z3_COUNTEREXAMPLE_GENERATION")
            
            self.log_interaction("Z3_COUNTEREXAMPLE_RESPONSE", iteration, "", response_content)
            
            return response_content
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆz3ä»£ç æ—¶å‡ºé”™: {e}")
            return ""
    
    def run_z3_solver(self, z3_code: str, ctx_folder: Path) -> Tuple[bool, str]:
        """
        è¿è¡ŒZ3æ±‚è§£å™¨
        
        Args:
            z3_code: Z3ä»£ç 
            ctx_folder: CTXæ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            (æ˜¯å¦æˆåŠŸ, è¾“å‡º)
        """
        try:
            # ä¿å­˜Z3ä»£ç 
            z3_file = ctx_folder / "z3_counterexample.py"
            z3_file.write_text(z3_code, encoding='utf-8')
            
            # è¿è¡ŒZ3æ±‚è§£å™¨
            result = subprocess.run(
                ['python3', str(z3_file)],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # ä¿å­˜ç»“æœ
            counterexample_file = ctx_folder / "counterexamples.txt"
            with open(counterexample_file, 'w', encoding='utf-8') as f:
                f.write("Z3 Solver Execution Results\n")
                f.write("=" * 25 + "\n")
                f.write(f"Return code: {result.returncode}\n")
                f.write(f"Output:\n{result.stdout}\n")
                if result.stderr:
                    f.write(f"Error:\n{result.stderr}\n")
            
            self.logger.info(f"Z3 files saved to: {ctx_folder}")
            
            return result.returncode == 0, result.stdout
                
        except subprocess.TimeoutExpired:
            return False, "Z3 solver timeout"
        except Exception as e:
            return False, f"Z3 solver error: {e}"
    
    def create_project_folder(self, original_file: Path) -> dict:
        """åˆ›å»ºé¡¹ç›®æ–‡ä»¶å¤¹ç»“æ„"""
        # åœ¨resultsç›®å½•ä¸‹åˆ›å»ºé¡¹ç›®æ–‡ä»¶å¤¹
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        project_name = original_file.stem
        project_folder = results_dir / project_name
        
        # åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„
        unverified_folder = project_folder / "unverified"
        verified_folder = project_folder / "verified"
        history_folder = project_folder / "history"
        ctx_folder = project_folder / "CTX"
        
        for folder in [unverified_folder, verified_folder, history_folder, ctx_folder]:
            folder.mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶åŸå§‹æ–‡ä»¶åˆ°unverifiedæ–‡ä»¶å¤¹
        original_copy = unverified_folder / original_file.name
        original_copy.write_text(original_file.read_text(encoding='utf-8'), encoding='utf-8')
        
        self.current_ctx_folder = ctx_folder
        
        return {
            'project_folder': project_folder,
            'unverified_folder': unverified_folder,
            'verified_folder': verified_folder,
            'history_folder': history_folder,
            'ctx_folder': ctx_folder
        }
    
    def save_iteration(self, code: str, iteration: int, history_folder: Path):
        """ä¿å­˜è¿­ä»£ç»“æœ"""
        iteration_file = history_folder / f"{iteration}.rs"
        iteration_file.write_text(code, encoding='utf-8')
        print(f"ä¿å­˜ç¬¬{iteration}æ¬¡è¿­ä»£åˆ°: {iteration_file}")
    
    def save_verified_result(self, code: str, verified_folder: Path):
        """ä¿å­˜éªŒè¯æˆåŠŸçš„ç»“æœ"""
        verified_file = verified_folder / "verified.rs"
        verified_file.write_text(code, encoding='utf-8')
        print(f"éªŒè¯æˆåŠŸï¼ä¿å­˜åˆ°: {verified_file}")

    def save_token_statistics(self, results_folder: Path):
        """ä¿å­˜tokenç»Ÿè®¡ä¿¡æ¯"""
        stats_file = results_folder / "token_statistics.json"
        
        # å‡†å¤‡ç»Ÿè®¡æ•°æ®
        stats_data = {
            "total_statistics": {
                "total_prompt_tokens": self.stats['total_prompt_tokens'],
                "total_completion_tokens": self.stats['total_completion_tokens'],
                "total_tokens": self.stats['total_tokens'],
                "api_calls": self.stats['api_calls'],
                "total_iterations": self.stats['total_iterations']
            },
            "iteration_statistics": self.stats['token_usage_by_iteration'],
            "verification_results": {
                "final_error_type": self.stats['final_error_type'],
                "compilation_errors": self.stats['compilation_errors'],
                "verification_errors": self.stats['verification_errors'],
                "z3_counterexample_generated": self.stats['z3_counterexample_generated'],
                "z3_counterexample_successful": self.stats['z3_counterexample_successful']
            }
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, indent=2, ensure_ascii=False)
        
        print(f"Tokenç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        
        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        print(f"\n=== TOKEN USAGE SUMMARY ===")
        print(f"æ€»APIè°ƒç”¨æ¬¡æ•°: {self.stats['api_calls']}")
        print(f"æ€»Prompt Tokens: {self.stats['total_prompt_tokens']:,}")
        print(f"æ€»Completion Tokens: {self.stats['total_completion_tokens']:,}")
        print(f"æ€»Tokens: {self.stats['total_tokens']:,}")
        if self.stats['api_calls'] > 0:
            print(f"å¹³å‡æ¯æ¬¡è°ƒç”¨Tokens: {self.stats['total_tokens'] // self.stats['api_calls']:,}")
        print("=" * 30)
    
    def process_file(self, file_path: str) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                self.logger.error(f"File not found: {file_path}")
                return False
        
            self.logger.info(f"=== Starting file processing: {file_path} ===")
        
            # è¯»å–åŸå§‹ä»£ç 
            original_code = file_path.read_text(encoding='utf-8')
            self.logger.info(f"\n--- ORIGINAL CODE ---")
            self.logger.info(original_code)
            self.logger.info(f"--- END ORIGINAL CODE ---")
            
            # åˆ›å»ºé¡¹ç›®æ–‡ä»¶å¤¹
            folders = self.create_project_folder(file_path)
            
            # åˆå§‹åŒ–å†å²è®°å½•
            history = []
            current_code = original_code
            
            print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
            
            # ä¸»å¾ªç¯
            for iteration in range(self.max_iterations + 1):
                self.stats['total_iterations'] = iteration
                
                if iteration == 0:
                    print(f"é˜¶æ®µ1ï¼šæ­£åœ¨ç”Ÿæˆåˆå§‹proof...")
                    self.logger.info(f"=== PHASE 1: INITIAL PROOF GENERATION ===")
                    
                    # ç”Ÿæˆåˆå§‹è¯æ˜
                    current_code = self.generate_initial_proof(original_code)
                    history.append(current_code)
                    
                else:
                    print(f"é˜¶æ®µ{iteration + 1}ï¼šç¬¬{iteration}æ¬¡éªŒè¯...")
                    
                    # ä¿å­˜å½“å‰è¿­ä»£
                    self.save_iteration(current_code, iteration, folders['history_folder'])
                    
                    print(f"  å­é˜¶æ®µ2.1ï¼šVeruséªŒè¯...")
                    
                    # éªŒè¯ä»£ç 
                    success, output, error = self.verify_with_verus(current_code, folders['history_folder'] / f"temp_{iteration}.rs")
            
                    if success:
                        print("éªŒè¯æˆåŠŸï¼")
                        self.logger.info(f"\n=== VERIFICATION SUCCESSFUL at iteration {iteration} ===")
                        self.stats['final_error_type'] = 'success'
                        
                        # ä¿å­˜éªŒè¯æˆåŠŸçš„ç»“æœ
                        self.save_verified_result(current_code, folders['verified_folder'])
                        self.save_token_statistics(folders['project_folder'])
                        
                        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
                        print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
                        print(f"ğŸ”„ æ€»è¿­ä»£æ¬¡æ•°: {iteration}")
                        print(f"ğŸ“ æœ€ç»ˆçŠ¶æ€: success")
                        print(f"ğŸ”§ ç¼–è¯‘é”™è¯¯æ¬¡æ•°: {self.stats['compilation_errors']}")
                        print(f"ğŸ” éªŒè¯é”™è¯¯æ¬¡æ•°: {self.stats['verification_errors']}")
                        print(f"ğŸ§® Z3åä¾‹ç”Ÿæˆå°è¯•: {'æ˜¯' if self.stats['z3_counterexample_generated'] else 'å¦'}")
                        print(f"âœ… Z3åä¾‹ç”ŸæˆæˆåŠŸ: {'æ˜¯' if self.stats['z3_counterexample_successful'] else 'å¦'}")
                        print(f"\nâœ… æ–‡ä»¶å¤„ç†æˆåŠŸï¼")
                        
                        self.logger.info(f"=== File processing completed successfully ===")
                        return True
                    
                    else:
                        print("éªŒè¯å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯:", error)
                        self.log_interaction("VERIFICATION_FAILED", iteration, "", "", error)
                        
                        # åˆ†æé”™è¯¯ç±»å‹
                        if self.is_compilation_error(error):
                            self.stats['compilation_errors'] += 1
                            print("  å­é˜¶æ®µ2.2ï¼šé”™è¯¯åˆ†æ...")
                            print("  å­é˜¶æ®µ2.3ï¼šæ£€æµ‹åˆ°ç¼–è¯‘é”™è¯¯ï¼Œè·³è¿‡åä¾‹ç”Ÿæˆ")
                            
                            # ç¼–è¯‘é”™è¯¯ä¸ç”Ÿæˆåä¾‹
                            self.stats['z3_counterexample_generated'] = False
                            self.stats['z3_counterexample_successful'] = False
                            
                        else:
                            self.stats['verification_errors'] += 1
                            print("  å­é˜¶æ®µ2.2ï¼šé”™è¯¯åˆ†æ...")
                            print("  å­é˜¶æ®µ2.3ï¼šæ£€æµ‹åˆ°è¯æ˜å¤±è´¥ï¼Œæ­£åœ¨ç”Ÿæˆåä¾‹...")
                            
                            # ç”Ÿæˆåä¾‹
                            self.stats['z3_counterexample_generated'] = True
                            z3_code = self.generate_z3_counterexample(current_code, error, iteration)
                            
                            if z3_code:
                                print("  å­é˜¶æ®µ2.4ï¼šè¿è¡ŒZ3æ±‚è§£å™¨...")
                                success, output = self.run_z3_solver(z3_code, folders['ctx_folder'])
                                
                                if success and output.strip():
                                    print("åä¾‹ç”ŸæˆæˆåŠŸï¼")
                                    self.stats['z3_counterexample_successful'] = True
                                    self.logger.info(f"\n--- COUNTEREXAMPLES GENERATED SUCCESSFULLY ---")
                                    self.logger.info(f"Counterexamples found: {len(output.strip().split('Counterexample')) - 1}")
                                    self.logger.info(f"--- END COUNTEREXAMPLES ---")
                                else:
                                    print("z3ä»£ç ç”Ÿæˆå¤±è´¥")
                                    self.stats['z3_counterexample_successful'] = False
                            else:
                                print("z3ä»£ç ç”Ÿæˆå¤±è´¥")
                                self.stats['z3_counterexample_successful'] = False
                        
                        # è®°å½•åä¾‹ç»Ÿè®¡
                        self.logger.info(f"=== COUNTEREXAMPLE STATISTICS ===")
                        self.logger.info(f"Counterexample generation attempted: {self.stats['z3_counterexample_generated']}")
                        self.logger.info(f"Counterexample generation successful: {self.stats['z3_counterexample_successful']}")
                        self.logger.info(f"=== END COUNTEREXAMPLE STATISTICS ===")
                        
                        print(f"  å­é˜¶æ®µ2.5ï¼šæ­£åœ¨è¿›è¡Œç¬¬{iteration}æ¬¡ä¿®å¤...")
                        
                        # ä¿®å¤è¯æ˜
                        fixed_code = self.fix_proof(current_code, error, history, output if 'output' in locals() else "", iteration)
                        current_code = fixed_code
                        history.append(current_code)
            
            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
            print(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°({self.max_iterations})ï¼Œæœªèƒ½æˆåŠŸéªŒè¯")
            self.logger.info(f"\n=== REACHED MAX ITERATIONS ({self.max_iterations}) - FAILED ===")
            
            if self.stats['compilation_errors'] > 0:
                self.stats['final_error_type'] = 'compilation_error'
            elif self.stats['verification_errors'] > 0:
                self.stats['final_error_type'] = 'verification_error'
            else:
                self.stats['final_error_type'] = 'unknown_error'
            
            self.save_token_statistics(folders['project_folder'])
            
            # æ‰“å°è¯¦ç»†ç»Ÿè®¡
            print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
            print(f"ğŸ”„ æ€»è¿­ä»£æ¬¡æ•°: {self.max_iterations}")
            print(f"ğŸ“ æœ€ç»ˆçŠ¶æ€: {self.stats['final_error_type']}")
            print(f"ğŸ”§ ç¼–è¯‘é”™è¯¯æ¬¡æ•°: {self.stats['compilation_errors']}")
            print(f"ğŸ” éªŒè¯é”™è¯¯æ¬¡æ•°: {self.stats['verification_errors']}")
            print(f"ğŸ§® Z3åä¾‹ç”Ÿæˆå°è¯•: {'æ˜¯' if self.stats['z3_counterexample_generated'] else 'å¦'}")
            print(f"âœ… Z3åä¾‹ç”ŸæˆæˆåŠŸ: {'æ˜¯' if self.stats['z3_counterexample_successful'] else 'å¦'}")
            print(f"\nâŒ æ–‡ä»¶å¤„ç†å¤±è´¥ï¼")
            
            self.logger.info(f"=== File processing completed with failure ===")
            return False
        
        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Verus Auto Prover')
    parser.add_argument('file', help='Rust/Verus file to process')
    parser.add_argument('--api-key', required=True, help='API key')
    parser.add_argument('--max-iterations', type=int, default=5, help='Maximum iterations')
    parser.add_argument('--use-gemini', action='store_true', default=False, help='Use Gemini API instead of OpenAI')
    
    args = parser.parse_args()
    
    # åˆ›å»ºproverå®ä¾‹
    prover = VerusAutoProver(
        api_key=args.api_key,
        max_iterations=args.max_iterations,
        use_gemini=args.use_gemini
    )
    
    # å¤„ç†æ–‡ä»¶
    success = prover.process_file(args.file)
    
    if success:
        print("âœ… å¤„ç†æˆåŠŸ")
        sys.exit(0)
    else:
        print("âŒ å¤„ç†å¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()