#!/usr/bin/env python3
"""
å…¨é¢çš„åŸºå‡†æµ‹è¯•è„šæœ¬
è¿è¡Œæ‰€æœ‰benchmarksæ–‡ä»¶å¹¶ç”Ÿæˆè¯¦ç»†ExcelæŠ¥å‘Š
"""

import os
import sys
import subprocess
import json
import re
from pathlib import Path
from datetime import datetime
import pandas as pd
from typing import List, Dict, Any

def extract_verus_error(error_output: str) -> str:
    """æå–ç®€åŒ–çš„Verusé”™è¯¯ä¿¡æ¯"""
    if not error_output:
        return ""
    
    # æå–å…³é”®é”™è¯¯ä¿¡æ¯
    lines = error_output.split('\n')
    error_lines = []
    
    for line in lines:
        if any(keyword in line.lower() for keyword in [
            'error:', 'failed', 'not satisfied', 'assertion failed',
            'invariant not satisfied', 'decreases not satisfied',
            'postcondition', 'precondition', 'verification failed'
        ]):
            # ç®€åŒ–é”™è¯¯ä¿¡æ¯
            simplified = line.strip()
            if len(simplified) > 80:
                simplified = simplified[:80] + "..."
            error_lines.append(simplified)
    
    return "; ".join(error_lines[:2])  # åªä¿ç•™å‰2ä¸ªé”™è¯¯

def extract_iteration_details(project_folder: Path, output: str) -> List[Dict[str, Any]]:
    """æå–æ¯æ¬¡è¿­ä»£çš„è¯¦ç»†ä¿¡æ¯"""
    iteration_details = []
    
    # è¯»å–tokenç»Ÿè®¡æ–‡ä»¶
    token_file = project_folder / "token_statistics.json"
    token_data = {}
    if token_file.exists():
        try:
            with open(token_file, 'r', encoding='utf-8') as f:
                token_data = json.load(f)
        except:
            pass
    
    # ä»è¾“å‡ºä¸­æå–æ¯æ¬¡è¿­ä»£çš„éªŒè¯ç»“æœ
    output_lines = output.split('\n')
    verification_errors = []
    
    # æŸ¥æ‰¾éªŒè¯é”™è¯¯
    for i, line in enumerate(output_lines):
        if "éªŒè¯å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯:" in line:
            error_start = i + 1
            error_lines = []
            # æ”¶é›†é”™è¯¯ä¿¡æ¯ç›´åˆ°ä¸‹ä¸€ä¸ªé˜¶æ®µ
            for j in range(error_start, min(error_start + 10, len(output_lines))):
                if output_lines[j].strip() and not output_lines[j].startswith("  å­é˜¶æ®µ"):
                    error_lines.append(output_lines[j].strip())
                else:
                    break
            verification_errors.append("\n".join(error_lines))
    
    # ä¸ºæ¯æ¬¡è¿­ä»£åˆ›å»ºè¯¦ç»†ä¿¡æ¯
    max_iterations = 5
    for iteration in range(1, max_iterations + 1):
        detail = {
            'iteration': iteration,
            'error_type': 'unknown',
            'verus_error': '',
            'counterexample_generated': False,
            'counterexample_count': 0,
            'token_cost': 0
        }
        
        # ä»tokenæ•°æ®ä¸­è·å–ä¿¡æ¯
        if str(iteration) in token_data.get('iteration_statistics', {}):
            iter_stats = token_data['iteration_statistics'][str(iteration)]
            detail['token_cost'] = iter_stats.get('total_tokens', 0)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åä¾‹ç”Ÿæˆ
            if 'Z3_COUNTEREXAMPLE_GENERATION' in iter_stats.get('steps', {}):
                detail['counterexample_generated'] = True
        
        # ä»éªŒè¯ç»“æœä¸­è·å–é”™è¯¯ç±»å‹
        if iteration - 1 < len(verification_errors):
            detail['verus_error'] = extract_verus_error(verification_errors[iteration - 1])
            
            # åˆ¤æ–­é”™è¯¯ç±»å‹
            error_text = verification_errors[iteration - 1].lower()
            if any(keyword in error_text for keyword in ['postcondition', 'precondition', 'assertion', 'invariant']):
                detail['error_type'] = 'verification_error'
            elif any(keyword in error_text for keyword in ['error[e', 'syntax error', 'type error']):
                detail['error_type'] = 'compilation_error'
        
        # æ£€æŸ¥åä¾‹æ–‡ä»¶
        ctx_folder = project_folder / "CTX"
        if ctx_folder.exists():
            counterexample_file = ctx_folder / "counterexamples.txt"
            if counterexample_file.exists():
                try:
                    content = counterexample_file.read_text(encoding='utf-8')
                    detail['counterexample_count'] = len(re.findall(r'counterexample', content.lower()))
                except:
                    pass
        
        iteration_details.append(detail)
    
    return iteration_details

def extract_counterexample_info(ctx_folder: Path) -> Dict[str, Any]:
    """æå–åä¾‹ä¿¡æ¯"""
    counterexample_file = ctx_folder / "counterexamples.txt"
    z3_file = ctx_folder / "z3_counterexample.py"
    
    info = {
        'counterexample_generated': False,
        'counterexample_successful': False,
        'counterexample_count': 0,
        'counterexample_content': ""
    }
    
    if z3_file.exists():
        info['counterexample_generated'] = True
        
        if counterexample_file.exists():
            try:
                content = counterexample_file.read_text(encoding='utf-8')
                info['counterexample_content'] = content
                
                # è®¡ç®—åä¾‹æ•°é‡
                counterexample_count = len(re.findall(r'counterexample', content.lower()))
                info['counterexample_count'] = counterexample_count
                info['counterexample_successful'] = counterexample_count > 0
            except:
                pass
    
    return info

def run_single_test_detailed(file_path: str, api_key: str) -> Dict[str, Any]:
    """
    è¿è¡Œå•ä¸ªæ–‡ä»¶çš„è¯¦ç»†æµ‹è¯•
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        api_key: APIå¯†é’¥
        
    Returns:
        è¯¦ç»†çš„æµ‹è¯•ç»“æœå­—å…¸
    """
    print(f"\n{'='*80}")
    print(f"ğŸ” å¼€å§‹è¯¦ç»†æµ‹è¯•: {file_path}")
    print(f"{'='*80}")
    
    try:
        # è¿è¡Œverus_auto_prover
        cmd = [
            "python3", "verus_auto_prover.py",
            "--api-key", api_key,
            file_path
        ]
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        # åˆ†æç»“æœ
        success = result.returncode == 0
        output = result.stdout
        error = result.stderr
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸéªŒè¯
        verification_success = "âœ… æ–‡ä»¶å¤„ç†æˆåŠŸï¼" in output
        verification_failed = "âŒ æ–‡ä»¶å¤„ç†å¤±è´¥ï¼" in output
        
        # æå–ç»Ÿè®¡ä¿¡æ¯
        stats = {}
        if "=== TOKEN USAGE SUMMARY ===" in output:
            lines = output.split('\n')
            for i, line in enumerate(lines):
                if "æ€»APIè°ƒç”¨æ¬¡æ•°:" in line:
                    stats['api_calls'] = line.split(':')[1].strip()
                elif "æ€»Tokens:" in line:
                    stats['total_tokens'] = line.split(':')[1].strip()
                elif "æ€»è¿­ä»£æ¬¡æ•°:" in line:
                    stats['iterations'] = line.split(':')[1].strip()
                elif "æœ€ç»ˆçŠ¶æ€:" in line:
                    stats['final_status'] = line.split(':')[1].strip()
        
        # è·å–é¡¹ç›®æ–‡ä»¶å¤¹ä¿¡æ¯
        project_name = Path(file_path).stem
        project_folder = Path("results") / project_name
        
        # æå–åä¾‹ä¿¡æ¯
        counterexample_info = {}
        if project_folder.exists():
            ctx_folder = project_folder / "CTX"
            if ctx_folder.exists():
                counterexample_info = extract_counterexample_info(ctx_folder)
        
        # æå–æ¯æ¬¡è¿­ä»£çš„è¯¦ç»†ä¿¡æ¯
        iteration_details = []
        if project_folder.exists():
            iteration_details = extract_iteration_details(project_folder, output)
        
        return {
            'file': file_path,
            'success': success,
            'verification_success': verification_success,
            'verification_failed': verification_failed,
            'stats': stats,
            'output': output,
            'error': error,
            'return_code': result.returncode,
            'counterexample_info': counterexample_info,
            'iteration_details': iteration_details,
            'project_folder': str(project_folder)
        }
        
    except subprocess.TimeoutExpired:
        return {
            'file': file_path,
            'success': False,
            'verification_success': False,
            'verification_failed': True,
            'stats': {},
            'output': '',
            'error': 'Timeout after 5 minutes',
            'return_code': -1,
            'counterexample_info': {},
            'iteration_details': [],
            'project_folder': ''
        }
    except Exception as e:
        return {
            'file': file_path,
            'success': False,
            'verification_success': False,
            'verification_failed': True,
            'stats': {},
            'output': '',
            'error': str(e),
            'return_code': -1,
            'counterexample_info': {},
            'iteration_details': [],
            'project_folder': ''
        }

def get_all_benchmark_files() -> List[str]:
    """è·å–æ‰€æœ‰benchmarkæ–‡ä»¶"""
    files = []
    
    # CloverBenchæ–‡ä»¶
    cloverbench_dir = Path("benchmarks/CloverBench/unverified")
    if cloverbench_dir.exists():
        for file in cloverbench_dir.glob("*.rs"):
            files.append(str(file))
    
    # Miscæ–‡ä»¶
    misc_dir = Path("benchmarks/Misc/unverified")
    if misc_dir.exists():
        for file in misc_dir.glob("*.rs"):
            files.append(str(file))
    
    # Diffyæ–‡ä»¶
    diffy_dir = Path("benchmarks/Diffy/unverified")
    if diffy_dir.exists():
        for file in diffy_dir.glob("*.rs"):
            files.append(str(file))
    
    # MBPPæ–‡ä»¶
    mbpp_dir = Path("benchmarks/MBPP/unverified")
    if mbpp_dir.exists():
        for file in mbpp_dir.glob("*.rs"):
            files.append(str(file))
    
    return files

def run_comprehensive_benchmark():
    """è¿è¡Œå…¨é¢çš„åŸºå‡†æµ‹è¯•"""
    
    # APIå¯†é’¥
    api_key = "your_openai_api_key_here"
    
    # è·å–æ‰€æœ‰benchmarkæ–‡ä»¶
    benchmark_files = get_all_benchmark_files()
    
    print("ğŸš€ å¼€å§‹å…¨é¢åŸºå‡†æµ‹è¯•")
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶æ•°é‡: {len(benchmark_files)}")
    
    # è¿è¡Œæµ‹è¯•
    results = []
    successful_tests = 0
    failed_tests = 0
    
    for i, file_path in enumerate(benchmark_files, 1):
        print(f"\nğŸ“Š è¿›åº¦: {i}/{len(benchmark_files)}")
        
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            continue
            
        result = run_single_test_detailed(file_path, api_key)
        results.append(result)
        
        if result['verification_success']:
            successful_tests += 1
            print(f"âœ… æµ‹è¯•æˆåŠŸ: {file_path}")
        else:
            failed_tests += 1
            print(f"âŒ æµ‹è¯•å¤±è´¥: {file_path}")
    
    # ç”ŸæˆExcelæŠ¥å‘Š
    generate_excel_report(results, successful_tests, failed_tests)
    
    return results

def generate_excel_report(results: List[Dict[str, Any]], successful_tests: int, failed_tests: int):
    """ç”ŸæˆExcelæŠ¥å‘Š"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    excel_file = f"comprehensive_benchmark_report_{timestamp}.xlsx"
    
    # å‡†å¤‡æ•°æ®
    data = []
    
    for result in results:
        # åŸºæœ¬ä¿¡æ¯
        file_name = Path(result['file']).name
        benchmark_type = Path(result['file']).parent.name
        
        # éªŒè¯ç»“æœ
        verification_status = "Success" if result['verification_success'] else "Failed"
        final_status = result['stats'].get('final_status', 'unknown')
        
        # ç»Ÿè®¡ä¿¡æ¯
        api_calls = result['stats'].get('api_calls', '0')
        total_tokens = result['stats'].get('total_tokens', '0')
        iterations = result['stats'].get('iterations', '0')
        
        # åŸºæœ¬è¡Œæ•°æ®
        row_data = {
            'Filename': file_name,
            'Benchmark_Type': benchmark_type,
            'Verification_Status': verification_status,
            'Final_Status': final_status,
            'Total_API_Calls': api_calls,
            'Total_Token_Usage': total_tokens,
            'Total_Iterations': iterations,
        }
        
        # ä¸ºæ¯æ¬¡è¿­ä»£æ·»åŠ è¯¦ç»†ä¿¡æ¯ (5æ¬¡è¿­ä»£ Ã— 5åˆ—ä¿¡æ¯ = 25åˆ—)
        iteration_details = result.get('iteration_details', [])
        for i in range(1, 6):  # è¿­ä»£1åˆ°5
            if i - 1 < len(iteration_details):
                detail = iteration_details[i - 1]
                row_data[f'Iteration{i}_Error_Type'] = detail.get('error_type', '')
                row_data[f'Iteration{i}_Verus_Error'] = detail.get('verus_error', '')
                row_data[f'Iteration{i}_Counterexample_Generated'] = 'Yes' if detail.get('counterexample_generated', False) else 'No'
                row_data[f'Iteration{i}_Counterexample_Count'] = detail.get('counterexample_count', 0)
                row_data[f'Iteration{i}_Token_Cost'] = detail.get('token_cost', 0)
            else:
                row_data[f'Iteration{i}_Error_Type'] = ''
                row_data[f'Iteration{i}_Verus_Error'] = ''
                row_data[f'Iteration{i}_Counterexample_Generated'] = ''
                row_data[f'Iteration{i}_Counterexample_Count'] = 0
                row_data[f'Iteration{i}_Token_Cost'] = 0
        
        # æ·»åŠ é¡¹ç›®æ–‡ä»¶å¤¹ä¿¡æ¯
        row_data['Project_Folder'] = result['project_folder']
        
        data.append(row_data)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(data)
    
    # åˆ›å»ºExcelå†™å…¥å™¨
    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
        # ä¸»æ•°æ®è¡¨
        df.to_excel(writer, sheet_name='è¯¦ç»†ç»“æœ', index=False)
        
        # ç»Ÿè®¡æ‘˜è¦è¡¨
        summary_data = {
            'æŒ‡æ ‡': [
                'æ€»æµ‹è¯•æ•°é‡',
                'æˆåŠŸæµ‹è¯•',
                'å¤±è´¥æµ‹è¯•',
                'æˆåŠŸç‡',
                'æ€»APIè°ƒç”¨',
                'æ€»Tokenä½¿ç”¨',
                'å¹³å‡æ¯æ¬¡è°ƒç”¨Tokens',
                'åä¾‹ç”ŸæˆæˆåŠŸç‡',
                'åä¾‹æˆåŠŸç”Ÿæˆç‡'
            ],
            'æ•°å€¼': [
                len(results),
                successful_tests,
                failed_tests,
                f"{successful_tests/len(results)*100:.1f}%" if len(results) > 0 else "0%",
                sum(int(r['stats'].get('api_calls', '0')) for r in results),
                sum(int(r['stats'].get('total_tokens', '0').replace(',', '')) for r in results),
                sum(int(r['stats'].get('total_tokens', '0').replace(',', '')) for r in results) // max(1, sum(int(r['stats'].get('api_calls', '0')) for r in results)),
                f"{sum(1 for r in results if r['counterexample_info'].get('counterexample_generated', False))}/{len(results)}",
                f"{sum(1 for r in results if r['counterexample_info'].get('counterexample_successful', False))}/{len(results)}"
            ]
        }
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_excel(writer, sheet_name='ç»Ÿè®¡æ‘˜è¦', index=False)
        
        # æŒ‰åŸºå‡†ç±»å‹åˆ†ç»„ç»Ÿè®¡
        benchmark_stats = df.groupby('åŸºå‡†ç±»å‹').agg({
            'éªŒè¯çŠ¶æ€': lambda x: (x == 'æˆåŠŸ').sum(),
            'æ–‡ä»¶å': 'count'
        }).rename(columns={'éªŒè¯çŠ¶æ€': 'æˆåŠŸæ•°é‡', 'æ–‡ä»¶å': 'æ€»æ•°é‡'})
        benchmark_stats['æˆåŠŸç‡'] = (benchmark_stats['æˆåŠŸæ•°é‡'] / benchmark_stats['æ€»æ•°é‡'] * 100).round(1)
        benchmark_stats.to_excel(writer, sheet_name='æŒ‰åŸºå‡†ç±»å‹ç»Ÿè®¡')
    
    print(f"\nğŸ“„ ExcelæŠ¥å‘Šå·²ä¿å­˜åˆ°: {excel_file}")
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*80}")
    print(f"ğŸ¯ å…¨é¢åŸºå‡†æµ‹è¯•å®Œæˆ")
    print(f"{'='*80}")
    print(f"ğŸ“Š æ€»æµ‹è¯•æ•°é‡: {len(results)}")
    print(f"âœ… æˆåŠŸæµ‹è¯•: {successful_tests}")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {failed_tests}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {successful_tests/len(results)*100:.1f}%")
    print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {excel_file}")

if __name__ == "__main__":
    run_comprehensive_benchmark()
